?ctree
newiris <- iris
newiris$Species <- NULL
(kc <- kmeans(newiris,3))
table(iris$Species, kc($cluster)
table(iris$Species, kc$cluster)
plot(newiris[c('Sepal.Length','Sepal.Width')], col=kc$cluster)
points(kc$centers[,c('Sepal.Length','Sepal.Width')],col=1:3,pch=8,cex=2)
png('kmeans.png')
plot(newiris[c('Sepal.Length','Sepal.Width')], col=kc$cluster)
points(kc$centers[,c('Sepal.Length','Sepal.Width')],col=1:3,pch=8,cex=2)
dev.off()
set.seed(1234)
ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.7, 0.3))
trainData <- iris[ind==1,]
testData <- iris[ind==2,]
ind
myFormula <- Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
iris_ctree <- ctree(myFormula, data=trainData)
table(predict(iris_ctree), trainData$Species)
print(iris_ctree)
plot(iris_ctree)
plot(iris_ctree, type="simple")
testPred <- predict(iris_ctree, newdata = testData)
table(testPred, testData$Species)
# by rpart
data("bodyfat", package = "mboost")
require(mboost)
data("bodyfat", package = "mboost")
dim(bodyfat)
set.seed(1234)
set.seed(1234)
ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.7, 0.3))
trainData <- iris[ind==1,]
testData <- iris[ind==2,]
library(rpart)
iris_rpart <- rpart(myFormula, data = trainData,
+ control = rpart.control(minsplit = 10))
iris_rpart <- rpart(myFormula, data = trainData,control = rpart.control(minsplit = 10))
attributes(iris_rpart)
named list()
print(iris_rpart$cptable)
print(iris_rpart)
plot(iris_rpart)
text(iris_rpart, use.n=T)
opt <- which.min(iris_rpart$cptable[,"xerror"])
iris_prune <- prune(iris_rpart, cp = cp)
cp <- iris_rpart$cptable[opt, "CP"]
iris_prune <- prune(iris_rpart, cp = cp)
print(iris_prune)
plot(bodyfat_prune)
plot(iris_prune)
text(iris_prune, use.n=T)
iris_pred <- predict(iris_prune, newdata=testData)
iris_pred <- predict(iris_prune, newdata=testData)
plot(iris_pred ~ ., data=testData, xlab="Observed",
+ ylab="Predicted", ylim=xlim, xlim=xlim)
plot(iris_pred ~ ., data=testData, xlab="Observed",ylab="Predicted", ylim=xlim, xlim=xlim)
xlim <- range(iris[,1])
plot(iris_pred ~ ., data=testData, xlab="Observed",ylab="Predicted", ylim=xlim, xlim=xlim)
iris_pred
testData
plot(iris_pred ~ Sepal.Length+Sepal.Width+Petal.Length+Petal.Width, data=testData, xlab="Observed",ylab="Predicted", ylim=xlim, xlim=xlim)
iris_pred <- predict(iris_prune, newdata=testData)
plot(iris_pred ~ Sepal.Length+Sepal.Width+Petal.Length+Petal.Width, data=testData, xlab="Observed",ylab="Predicted")
abline(a=0, b=1)
iris_pred <- predict(iris_prune, newdata=testData)
iris_pred
plot(iris_pred[,1] ~ Sepal.Length+Sepal.Width+Petal.Length+Petal.Width, data=testData, xlab="Observed",ylab="Predicted")
plot(iris_pred[,1] ~ Sepal.Length+Sepal.Width+Petal.Length+Petal.Width, data=testData, xlab="Observed",ylab="Predicted")
abline(a=0, b=1)
ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.7, 0.3))
trainData <- iris[ind==1,]
testData <- iris[ind==2,]
library(randomForest)
rf <- randomForest(Species ~ ., data=trainData, ntree=100, proximity=TRUE)
table(predict(rf), trainData$Species)
print(rf)
attributes(rf)
plot(rf)
png('random_forest.png')
plot(rf)
dev.off()
importance(rf)
varImpPlot(rf)
irisPred <- predict(rf, newdata=testData)
table(irisPred, testData$Species)
plot(margin(rf, testData$Species))
png('random_forest_2.png')
plot(margin(rf, testData$Species))
dev.off()
function (package, help, pos = 2, lib.loc = NULL, character.only = FALSE,
library(fpc)
setwd('C:\\Users\\Ivan.Liuyanfeng\\Desktop\\ata_Mining_Work_Space\\rdatamining')
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
install.packages('AppliedPredictiveModeling')
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
adData
diagnosis
predictors
head(diagnosis)
head(predictors)
adData = data.frame(diagnosis,predictors)
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
head(testIndex)
testIndex = createDataPartition(diagnosis, p = 0.50,list=T)
head(testIndex)
head(testIndex)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
data(concrete)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
head(training)
require(Hmisc)
a <- cut2(inTrain)
a
concrete$a <- cut2(inTrain)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
concrete$SuperPlasticizer
concrete$SuperPlasticizer
colnames(concrete)
concrete$Superplasticizer
hist(concrete$Superplasticizer)
hist(log(concrete$Superplasticizer))
hist(concrete$Superplasticizer)
par(mfcol=c(1,2))
hist(log(concrete$Superplasticizer))
hist(concrete$Superplasticizer)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
head(AlzheimerDisease)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
head(training)
str(training)
str(training[,57:68])
str(training[,58:69])
preProcess(training[,58:69], method='pca')
pca <- preProcess(training[,58:69], method='pca')
plot(pca)
predict(training[,58:69],pca)[,1]
predict(pca,training[,58:69])[,1]
predict(pca,training[,58:69])
pca.p <- predict(pca,training[,58:69])
plot(pca.p)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
concrete$Superplasticizer
par(mfcol=c(1,2))
hist(log(concrete$Superplasticizer))
hist(concrete$Superplasticizer)
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
testIndex = createDataPartition(diagnosis, p = 0.50,list=F)
head(testIndex)
data(concrete)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
head(training)
require(Hmisc)
plot(training$Compre
plot(training$CompressiveStrength,pch=1,col=cut2(training$FlyAsh,m=20))
cut2(training$FlyAsh
)
plot(training$CompressiveStrength,pch=1,col=cut2(training$Age,m=20))
plot(training$CompressiveStrength,pch=1,col=cut2(training$Cement,m=20))
pca <- preProcess(training[,58:69], method='pca', pcaComp=2)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
str(training[,58:69])
pca <- preProcess(training[,58:69], method='pca', pcaComp=2)
pca.p <- predict(pca,training[,58:69])
plot(pca.p)
pca.p
pca <- preProcess(training[,58:69], method='pca')
pca.p <- predict(pca,training[,58:69])
plot(pca.p)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
str(training[,58:69])
pca <- preProcess(training[,58:69], method='pca')
pca.p <- predict(pca,training[,58:69])
plot(pca.p)
prcomp(training[,58:69])
pca$rotation
plot(prcomp(training[,58:69]))
svd1 <- svd(prcomp(training[,58:69]))
svd1 <- svd(scale(prcomp(training[,58:69])))
scale(prcomp(training[,58:69]))
svd1 <- svd(scale(training[,58:69]))
svd1
plot(svd1$d)
plot(svd1$d^2/sum(svd1$d^2))
svd1 <- svd(training[,58:69])
plot(svd1$d^2/sum(svd1$d^2))
scale
svd1 <- svd(scale(training[,58:69]))
plot(svd1$d^2/sum(svd1$d^2))
pca[,1]
pca
pca$rotation
plot(svd1$d^2/sum(svd1$d^2))
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
require(e1071)
fit <- svm(CompressiveStrength~., training)
pred <- predict(fit, testing)
# confusionMatrix(as.vector(pred), testing$CompressiveStrength)
sum(sqrt((pred-testing$CompressiveStrength)^2))
library(caret)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
require(e1071)
fit <- svm(CompressiveStrength~., training)
pred <- predict(fit, testing)
# confusionMatrix(as.vector(pred), testing$CompressiveStrength)
sum(sqrt((pred-testing$CompressiveStrength)^2))
sqrt(sum((pred-testing$CompressiveStrength)^2))
sqrt(107.44)
?colSums
?dgamma
?lm
?predict
?dgamma
?predict
?dgamma
setwd("C:\\Users\\Ivan.Liuyanfeng\\Desktop\\Data_Mining_Work_Space\\Data-Science-Statistical-Inference")
n <- 5
pvals <- seq(0, 1, length = 1000)
plot(c(0, 1), c(0, 1.2), type = "n", frame = FALSE, xlab = "p", ylab = "likelihood")
text((0 : n) /n, 1.1, as.character(0 : n))
sapply(0 : n, function(x) {
phat <- x / n
if (x == 0) lines(pvals,  ( (1 - pvals) / (1 - phat) )^(n-x), lwd = 3)
else if (x == n) lines(pvals, (pvals / phat) ^ x, lwd = 3)
else lines(pvals, (pvals / phat ) ^ x * ( (1 - pvals) / (1 - phat) ) ^ (n-x), lwd = 3)
}
nosim <- 1000
nosim <- 1000
n <- 10
sd(apply(matrix(rnorm(nosim * n), nosim), 1, mean))
rnorm(nosim * n)
matrix(rnorm(nosim * n), nosim
apply(matrix(rnorm(nosim * n), nosim)
matrix(rnorm(nosim * n), nosim)
apply(matrix(rnorm(nosim * n), nosim), 1, mean)
sd(apply(matrix(rnorm(nosim * n), nosim), 1, mean))
1 / sqrt(n)
nosim <- 1000
n <- 10
sd(apply(matrix(runif(nosim * n), nosim), 1, mean))
runif(nosim * n)
?runif
?rnorm
1 / sqrt(12 * n)
sd(apply(matrix(runif(nosim * n), nosim), 1, mean))
nosim <- 1000
n <- 10
?sample
sample(0 : 1, nosim * n, replace = TRUE)
matrix(sample(0 : 1, nosim * n, replace = TRUE),
nosim)
apply(matrix(sample(0 : 1, nosim * n, replace = TRUE),
nosim), 1, mean)
sd(apply(matrix(sample(0 : 1, nosim * n, replace = TRUE),
nosim), 1, mean))
1 / (2 * sqrt(n))
library(UsingR); data(father.son);
x <- father.son$sheight
x
n<-length(x)
n
library(ggplot2)
qplot(x)
qplot()+geom_density(x)
qplot(x)+geom_density(x)
qplot(x)+geom_line(x)
qplot(x)
qplot(binwidth=x)+geom_line(x)
qplot(binwidth=x)
qplot(x, binwidth=x)
qplot(x)
round(c(var(x), var(x) / n, sd(x), sd(x) / sqrt(n)),2)
choose(8, 7) * 0.5^8 + choose(8, 8) * 0.5^8
choose(8, 7)
choose(8, 8)
choose(8, 7) * 0.5
choose(8, 7) * 0.5^8
pbinom(6, size = 8, prob = 0.5, lower.tail = FALSE)
?pbinom
qnorm(.95, mean = mu, sd = sd)
qnorm(.95, mean = 0, sd = 1)
pnorm(1160, mean = 1020, sd = 50, lower.tail = FALSE)
pnorm(2.8, lower.tail = FALSE)
pnorm(.75, mean = 1020, sd = 50, lower.tail = FALSE)
qnorm(.75, mean = 1020, sd = 50, lower.tail = FALSE)
qnorm(.75, mean = 1020, sd = 50)
qnorm(.75, mean = 1020, sd = 50, lower.tail = T)
ppois(3, lambda = 2.5 * 4)
pbinom(2, size = 500, prob = 0.01)
ppois(2, lambda = 500 * 0.01)
n <- 10000
means <- cumsum(rnorm(n))/(1:n)
means
cumsum(rnorm(n))
?cumsum
means <- cumsum(rnorm(n))/(1:n)
means
library(ggplot2)
g <- ggplot(data.frame(x = 1:n, y = means), aes(x = x, y = y))
g <- g + geom_hline(yintercept = 0) + geom_line(size = 2)
g <- g + labs(x = "Number of obs", y = "Cumulative mean")
g
png('asymptopia.png')
g
dev.off()
means <- cumsum(sample(0:1, n, replace = TRUE))/(1:n)
g <- ggplot(data.frame(x = 1:n, y = means), aes(x = x, y = y))
g <- g + geom_hline(yintercept = 0.5) + geom_line(size = 2)
g <- g + labs(x = "Number of obs", y = "Cumulative mean")
g
png('asymptopia2.png')
g
dev.off()
library(UsingR)
data(father.son)
x <- father.son$sheight
(mean(x) + c(-1, 1) * qnorm(0.975) * sd(x)/sqrt(length(x)))/12
round(1/sqrt(10^(1:6)), 3)
0.56 + c(-1, 1) * qnorm(0.975) * sqrt(0.56 * 0.44/100)
binom.test(56, 100)$conf.int
n <- 20
pvals <- seq(0.1, 0.9, by = 0.05)
pvals
nosim <- 1000
coverage <- sapply(pvals, function(p) {
phats <- rbinom(nosim, prob = p, size = n)/n
ll <- phats - qnorm(0.975) * sqrt(phats * (1 - phats)/n)
ul <- phats + qnorm(0.975) * sqrt(phats * (1 - phats)/n)
mean(ll < p & ul > p)
})
qplot(coverage)
g<-ggplot(aes(x=pvals,y=coverage))
coverage
pvals
df <- data.frame(pvals=pvals,coverage=coverage)
df
qplot(df,aes(x=pvals,y=coverage))
ggplot(df,aes(x=pvals,y=coverage))
g <- ggplot(df,aes(x=pvals,y=coverage))
g + geom_line(size=2)
png('simulation.png')
g + geom_line(size=2)
dev.off()
g + geom_line(size=2) + geom_hline(yintercept=.95)
png('simulation.png')
g + geom_line(size=2) + geom_hline(yintercept=.95)
dev.off()
n <- 100
pvals <- seq(0.1, 0.9, by = 0.05)
nosim <- 1000
coverage2 <- sapply(pvals, function(p) {
phats <- rbinom(nosim, prob = p, size = n)/n
ll <- phats - qnorm(0.975) * sqrt(phats * (1 - phats)/n)
ul <- phats + qnorm(0.975) * sqrt(phats * (1 - phats)/n)
mean(ll < p & ul > p)
})
df <- data.frame(pvals=pvals,coverage=coverage)
g <- ggplot(df,aes(x=pvals,y=coverage))
g + geom_line(size=2) + geom_hline(yintercept=.95)
df2 <- data.frame(pvals=pvals,coverage=coverage2)
g2 <- ggplot(df2,aes(x=pvals,y=coverage))
png('simulation2.png')
g + geom_line(size=2) + geom_hline(yintercept=.95)
dev.off()
g + geom_line(size=2) + geom_hline(yintercept=.95)
g2 <- ggplot(df2,aes(x=pvals,y=coverage2))
g + geom_line(size=2) + geom_hline(yintercept=.95)
df2 <- data.frame(pvals=pvals,coverage=coverage2)
df2
df
g2 <- ggplot(df2,aes(x=pvals,y=coverage2))
g2 + geom_line(size=2) + geom_hline(yintercept=.95)
png('simulation2.png')
g2 + geom_line(size=2) + geom_hline(yintercept=.95)
dev.off()
n <- 20
pvals <- seq(0.1, 0.9, by = 0.05)
nosim <- 1000
coverage <- sapply(pvals, function(p) {
phats <- (rbinom(nosim, prob = p, size = n) + 2)/(n + 4)
ll <- phats - qnorm(0.975) * sqrt(phats * (1 - phats)/n)
ul <- phats + qnorm(0.975) * sqrt(phats * (1 - phats)/n)
mean(ll < p & ul > p)
})
df3 <- data.frame(pvals=pvals,coverage=coverage)
g3 <- ggplot(df,aes(x=pvals,y=coverage))
g3 + geom_line(size=2) + geom_hline(yintercept=.95)
n <- 20
pvals <- seq(0.1, 0.9, by = 0.05)
nosim <- 1000
coverage <- sapply(pvals, function(p) {
phats <- (rbinom(nosim, prob = p, size = n) + 2)/(n + 4)
ll <- phats - qnorm(0.975) * sqrt(phats * (1 - phats)/n)
ul <- phats + qnorm(0.975) * sqrt(phats * (1 - phats)/n)
mean(ll < p & ul > p)
})
df3 <- data.frame(pvals=pvals,coverage=coverage)
g3 <- ggplot(df,aes(x=pvals,y=coverage))
png('simulation3.png')
g3 + geom_line(size=2) + geom_hline(yintercept=.95)
dev.off()
g3 + geom_line(size=2) + geom_hline(yintercept=.95)
g3 <- ggplot(df3,aes(x=pvals,y=coverage))
g3 + geom_line(size=2) + geom_hline(yintercept=.95)
png('simulation3.png')
g3 + geom_line(size=2) + geom_hline(yintercept=.95)
dev.off()
x <- 5
t <- 94.32
lambda <- x/t
round(lambda + c(-1, 1) * qnorm(0.975) * sqrt(lambda/t), 3)
poisson.test(x, T = 94.32)$conf
lambdavals <- seq(0.005, 0.1, by = 0.01)
nosim <- 1000
t <- 100
coverage <- sapply(lambdavals, function(lambda) {
lhats <- rpois(nosim, lambda = lambda * t)/t
ll <- lhats - qnorm(0.975) * sqrt(lhats/t)
ul <- lhats + qnorm(0.975) * sqrt(lhats/t)
mean(ll < lambda & ul > lambda)
})
df4 <- data.frame(lambdavals=lambdavals,coverage=coverage)
g4 <- ggplot(df4,aes(x=lambdavals,y=coverage))
g4 + geom_line(size=2) + geom_hline(yintercept=.95)
png('poissonsimulation.png')
g4 + geom_line(size=2) + geom_hline(yintercept=.95)
dev.off()
lambdavals <- seq(0.005, 0.1, by = 0.01)
nosim <- 1000
t <- 1000
coverage <- sapply(lambdavals, function(lambda) {
lhats <- rpois(nosim, lambda = lambda * t)/t
ll <- lhats - qnorm(0.975) * sqrt(lhats/t)
ul <- lhats + qnorm(0.975) * sqrt(lhats/t)
mean(ll < lambda & ul > lambda)
})
df4 <- data.frame(lambdavals=lambdavals,coverage=coverage)
g4 <- ggplot(df4,aes(x=lambdavals,y=coverage))
g4 + geom_line(size=2) + geom_hline(yintercept=.95)
png('poissonsimulation2.png')
g4 + geom_line(size=2) + geom_hline(yintercept=.95)
dev.off()
